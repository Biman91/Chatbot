{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import random\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file\n",
    "file = open('practice.txt', 'r')\n",
    "file = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spaCy', 'features', 'a', 'rule-matching', 'engine', ',', 'the', 'Matcher', ',', 'that', 'operates', 'over', 'tokens', ',', 'similar', 'to', 'regular', 'expressions', '.', 'The', 'rules', 'can', 'refer', 'to', 'token', 'annotations', '(', 'e.g', '.', 'the', 'token', 'text', 'or', 'tag_', ',', 'and', 'flags', '(', 'e.g', '.', 'IS_PUNCT', ')', '.', 'The', 'rule', 'matcher', 'also', 'lets', 'you', 'pass', 'in', 'a', 'custom', 'callback', 'to', 'act', 'on', 'matches', '–', 'for', 'example', ',', 'to', 'merge', 'entities', 'and', 'apply', 'custom', 'labels', '.', 'You', 'can', 'also', 'associate', 'patterns', 'with', 'entity', 'IDs', ',', 'to', 'allow', 'some', 'basic', 'entity', 'linking', 'or', 'disambiguation', '.', 'To', 'match', 'large', 'terminology', 'lists', ',', 'you', 'can', 'use', 'the', 'PhraseMatcher', ',', 'which', 'accepts', 'Doc', 'objects', 'as', 'match', 'patterns', '.', 'Compared', 'to', 'using', 'regular', 'expressions', 'on', 'raw', 'text', ',', 'spaCy', '’', 's', 'rule-based', 'matcher', 'engines', 'and', 'components', 'not', 'only', 'let', 'you', 'find', 'you', 'the', 'words', 'and', 'phrases', 'you', '’', 're', 'looking', 'for', '–', 'they', 'also', 'give', 'you', 'access', 'to', 'the', 'tokens', 'within', 'the', 'document', 'and', 'their', 'relationships', '.', 'This', 'means', 'you', 'can', 'easily', 'access', 'and', 'analyze', 'the', 'surrounding', 'tokens', ',', 'merge', 'spans', 'into', 'single', 'tokens', 'or', 'add', 'entries', 'to', 'the', 'named', 'entities', 'in', 'doc.ents', '.', 'my', 'name', 'is', 'Biman', 'Pal', '.', 'My', 'name', 'is', 'Abhay', '.', 'Abhay', 'am', 'working', 'with', 'Infosys', '.', 'My', 'name', 'is', 'Shubham', 'Mondal', '.', 'Shubham', 'working', 'with', 'Nihilent', '.', 'My', 'name', 'is', 'Sweta', 'Kumari', '.', 'Sweta', 'is', 'working', 'in', 'Nihilent', '.', 'Sweta', 'is', 'in', 'Multichoice', 'project', '.']\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "tokens = nltk.word_tokenize(file)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spaCy', 'features', 'a', 'rule-matching', 'engine', ',', 'the', 'Matcher', ',', 'that', 'operates', 'over', 'tokens', ',', 'similar', 'to', 'regular', 'expressions', '.', 'The', 'rules', 'can', 'refer', 'to', 'token', 'annotations', '(', 'e.g', '.', 'the', 'token', 'text', 'or', 'tag_', ',', 'and', 'flags', '(', 'e.g', '.', 'IS_PUNCT', ')', '.', 'The', 'rule', 'matcher', 'also', 'lets', 'you', 'pass', 'in', 'a', 'custom', 'callback', 'to', 'act', 'on', 'matches', '–', 'for', 'example', ',', 'to', 'merge', 'entities', 'and', 'apply', 'custom', 'labels', '.', 'You', 'can', 'also', 'associate', 'patterns', 'with', 'entity', 'IDs', ',', 'to', 'allow', 'some', 'basic', 'entity', 'linking', 'or', 'disambiguation', '.', 'To', 'match', 'large', 'terminology', 'lists', ',', 'you', 'can', 'use', 'the', 'PhraseMatcher', ',', 'which', 'accepts', 'Doc', 'objects', 'as', 'match', 'patterns', '.', 'Compared', 'to', 'using', 'regular', 'expressions', 'on', 'raw', 'text', ',', 'spaCy', '’', 's', 'rule-based', 'matcher', 'engines', 'and', 'components', 'not', 'only', 'let', 'you', 'find', 'you', 'the', 'words', 'and', 'phrases', 'you', '’', 're', 'looking', 'for', '–', 'they', 'also', 'give', 'you', 'access', 'to', 'the', 'tokens', 'within', 'the', 'document', 'and', 'their', 'relationships', '.', 'This', 'means', 'you', 'can', 'easily', 'access', 'and', 'analyze', 'the', 'surrounding', 'tokens', ',', 'merge', 'spans', 'into', 'single', 'tokens', 'or', 'add', 'entries', 'to', 'the', 'named', 'entities', 'in', 'doc.ents', '.', 'my', 'name', 'is', 'Biman', 'Pal', '.', 'My', 'name', 'is', 'Abhay', '.', 'Abhay', 'am', 'working', 'with', 'Infosys', '.', 'My', 'name', 'is', 'Shubham', 'Mondal', '.', 'Shubham', 'working', 'with', 'Nihilent', '.', 'My', 'name', 'is', 'Sweta', 'Kumari', '.', 'Sweta', 'is', 'working', 'in', 'Nihilent', '.', 'Sweta', 'is', 'in', 'Multichoice', 'project', '.']\n"
     ]
    }
   ],
   "source": [
    "#Create a function to return a list of lemmatized lower case words after removing punctuations\n",
    "def LemNormalize(text):\n",
    "  return nltk.word_tokenize(text)\n",
    "\n",
    "#Print the tokenization text\n",
    "print(LemNormalize(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spaCy features a rule-matching engine, the Matcher, that operates over tokens, similar to regular expressions.', 'The rules can refer to token annotations (e.g.', 'the token text or tag_, and flags (e.g.', 'IS_PUNCT).', 'The rule matcher also lets you pass in a custom callback to act on matches – for example, to merge entities and apply custom labels.', 'You can also associate patterns with entity IDs, to allow some basic entity linking or disambiguation.', 'To match large terminology lists, you can use the PhraseMatcher, which accepts Doc objects as match patterns.', 'Compared to using regular expressions on raw text, spaCy’s rule-based matcher engines and components not only let you find you the words and phrases you’re looking for – they also give you access to the tokens within the document and their relationships.', 'This means you can easily access and analyze the surrounding tokens, merge spans into single tokens or add entries to the named entities in doc.ents.', 'my name is Biman Pal.', 'My name is Abhay.', 'Abhay am working with Infosys.', 'My name is Shubham Mondal.', 'Shubham working with Nihilent.', 'My name is Sweta Kumari.', 'Sweta is working in Nihilent.', 'Sweta is in Multichoice project.']\n"
     ]
    }
   ],
   "source": [
    "# tokenize by sententence\n",
    "sent_tokens = nltk.sent_tokenize(file)\n",
    "#print sentences\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "{33: None, 34: None, 35: None, 36: None, 37: None, 38: None, 39: None, 40: None, 41: None, 42: None, 43: None, 44: None, 45: None, 46: None, 47: None, 58: None, 59: None, 60: None, 61: None, 62: None, 63: None, 64: None, 91: None, 92: None, 93: None, 94: None, 95: None, 96: None, 123: None, 124: None, 125: None, 126: None}\n"
     ]
    }
   ],
   "source": [
    "#Create a dictionary (key:value) pair to remove punctuations\n",
    "remove_punct_dict = dict(  ( ord(punct),None) for punct in string.punctuation)\n",
    "\n",
    "#Print the punctuations\n",
    "print(string.punctuation)\n",
    "\n",
    "#Print the dictionary\n",
    "print(remove_punct_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword Matching\n",
    "\n",
    "#Greeting Inputs\n",
    "GREETING_INPUTS = [\"hi\", \"hello\", \"hola\", \"greetings\", \"wassup\", \"hey\"]\n",
    "\n",
    "#Greeting responses back to the user\n",
    "GREETING_RESPONSES=[\"howdy\", \"hi\", \"hey\", \"what's good\", \"hello\", \"hey there\"]\n",
    "\n",
    "#Function to return a random greeting response to a users greeting\n",
    "def greeting(sentence):\n",
    "  #if the user's input is a greeting, then return a randomly chosen greeting response\n",
    "  for word in sentence.split():\n",
    "    if word.lower() in GREETING_INPUTS:\n",
    "      return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate USER Response\n",
    "def response(user_response):\n",
    "    # getting response from user\n",
    "    user_response = user_response.lower() # conver user response in lower case\n",
    "    \n",
    "    # spacy bot response empty, we will add later\n",
    "    robo_response = ''\n",
    "    \n",
    "    # append user response in the text file\n",
    "    sent_tokens.append(user_response)\n",
    "    \n",
    "    # create vectorize object and transform into vectorize format\n",
    "    TfidfVec = TfidfVectorizer(tokenizer = LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    \n",
    "    # compare user response with complete text in cosine similarity\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    \n",
    "    # get index of most similar sentence\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    \n",
    "    # convert into 1 demention from 2 dimention and sort the value.\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    \n",
    "    # get similar sentence of user , -1 is user input.\n",
    "    score = flat[-2]\n",
    "    \n",
    "    # if no match with user responce and append in robo response if there is match\n",
    "    if(score == 0):\n",
    "        robo_response = robo_response+\"I apologize, I don't understand.\"\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "    \n",
    "    # remove user response from sentense\n",
    "    sent_tokens.remove(user_response)\n",
    "    \n",
    "    # return robo response\n",
    "    return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpacyBot: I am bot.\n",
      "hi\n",
      "SpacyBot: hey there\n",
      "what is sweta name\n",
      "SpacyBot: My name is Sweta Kumari.\n",
      "where is sweta working\n",
      "SpacyBot: Sweta is working in Nihilent.\n",
      "what is sweta project\n",
      "SpacyBot: Sweta is in Multichoice project.\n",
      "what is spacy\n",
      "SpacyBot: spaCy features a rule-matching engine, the Matcher, that operates over tokens, similar to regular expressions.\n",
      "bye\n",
      "spacyBot: Chat with you later !\n"
     ]
    }
   ],
   "source": [
    "# create bot to get user response\n",
    "flag = True\n",
    "print(\"SpacyBot: I am bot.\")\n",
    "\n",
    "# bot started\n",
    "while(flag == True):\n",
    "    # get user response\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower() # conver into lower\n",
    "    \n",
    "    # condition of user response\n",
    "    if(user_response != 'bye'):\n",
    "        if(user_response == 'thanks' or user_response =='thank you'):\n",
    "            flag=False\n",
    "            print(\"SpacyBot: You are welcome !\")\n",
    "        else:\n",
    "            if(greeting(user_response) != None):\n",
    "                print(\"SpacyBot: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"SpacyBot: \"+response(user_response))       \n",
    "    else:\n",
    "        flag = False\n",
    "        print(\"spacyBot: Chat with you later !\")\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
